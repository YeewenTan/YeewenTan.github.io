---
layout: ja-default
title: "ディープニューラルネットワークについて"
redirect_from: /ja-neuralnet-overview
---

# ディープニューラルネットワークについて

目次

* <a href="#define">ニューラルネットワークとは？</a>
* <a href="#element">ニューラルネットワークの構成要素</a>
* <a href="#concept">ディープ・ニューラル・ネットワークの重要概念</a>
* <a href="#forward">例：フィードフォワードネットワーク＆誤差逆伝播法</a>
* <a href="#regression">多重線形回帰</a>
* <a href="#logistic">ロジスティック回帰＆分類</a>
* <a href="#ai">ニューラルネットワーク＆人口知能（AI）</a>
* <a href="#intro">その他の説明ガイドリソース</a>

## <a name="define"></a>

ニューラルネットワークは、人間の脳を模倣したアルゴリズムの一式で、パターン認識をするように設計されています。一種の機械知覚により感覚データを解釈し、生の入力情報にラベルを付け、クラスタリングします。その認識するパターンは数値で表されたもので、ベクトルにあります。実際の社会にあるすべてのデータは、画像、音、テキスト、時系列であっても、このような数値に変換されます。

ニューラルネットワークは、人間に代わってデータをクラスタリングし、分類します。そして、ニューラルネットワークは層を成していて、クラスタリングや分類作業を保管や管理されるデータの上で行っていると思っていただいて結構です。exampleの入力情報を、類似性に基づいてラベル付けされていないデータをグループ化します。そして、ラベル付けが終了し、トレーニング可能なデータの一式が準備できると、データを分類します。（正確に言うと、ニューラルネットワークが特徴を抽出すると、それらの特徴が他のアルゴリズムに入力され、クラスタリングや分類が行われます。従って、ディープ・ニューラル・ネットワークは、[強化学習（reinforcement learning）](../reinforcementlearning.html)、分類、[回帰（regression）](ja-linear-regression.html)を行うためのアルゴリズムが導入された、より大型の機械学習アプリケーションのコンポーネントであると考えていただいていいでしょう。）

ディープラーニングが扱うことができる課題を一つ考える際には、どんなカテゴリーが気になっているのか、どの情報に基づいて行動するのがいいと思うか、考えてみてください。その答えが、データに適用されるラベルに当たります。例えば、`spam`か`not_spam`、`good_guy`か`bad_guy`、`angry_customer`か`happy_customer`などです。次に、自分は、それらのラベルを伴うデータを持っているか、ラベルの付いたデータを見つけることができるか、ラベルの付いたデータセットを作成することができるか、考えてみてください。

例えば、癌に罹る恐れのある人々のグループを知りたい場合、トレーニングするグループの例として挙げられるのは、癌に罹った人々と癌に罹っていない人々、そしてそれらの人々それぞれのIDに関連したすべてのデータがあります。IDには、年齢、喫煙習慣の有無などの明示的な特徴から、ライフスタイル、習慣、興味、リスクなどがよく分かる行動を追跡した時系列などの生データ、オンライン行動の記録までも含まれます。

準備したデータセットを使って、ニューラルネットワークをトレーニングし、人々が癌患者か否かで分類し、その分類モデルを使って、癌のリスクが未知の人々に適用し、癌のリスクを予測し、リスクがある人々により注意を向け、予防対策を立てることができるのです。

恋愛のパートナー、未来のメジャーリーグのスーパースター、将来有望な社員、有望性の望めない俳優などもトレーニングを行って同じような過程で割り出すことができるのです。データは、必要不可欠な統計やソーシャルグラフ、生のテキストによるコミュニケーション、などを集め、パターンや関連した人々を見つけるためにデータを比較します。

## <a name="element">ニューラルネットワークの構成要素</a>

ディープラーニングとは、あるまとまった数の層で構成されたニューラルネットワークのことです。そしてその層は、ノード（節）で構成されています。ノードは計算を行いますが、人間の神経細胞を模倣したもので、十分な量の刺激を受けると、発火します。また、ノードに入力すると一式の係数、または重みといって、入力情報の強化、減退を行うものが結合します。このような方法で、アルゴリズムが学習するタスクの中で、入力情報の重要性が割り当てられます。次に、複数の重みが結合した入力の総和がノードの活性化機能と呼ばれる部分を通過します。これにより、その信号が、ネット内を進んでいくのか、そうであればどの程度進み、最終的な結果に影響するのか、などのような分類作業が行われます。

下図は、ノードがどのようなものかを表したものです。

![Alt text](../img/perceptron_node.png)

ノードの層には、ネットワークに入力されると、オン、オフと切り替える神経細胞のスイッチのようなものが並んでいます。また、各層の出力は、次の層の入力となります。最初の層が、人間の入力したデータを受け取ります。  

![Alt text](../img/mlp.png)

入力した特徴に調整可能な重みを付与することにより、その特徴に重要性の度合いを割り当て、これを基にしてネットワークは入力を分類し、クラスタリングします。

## <a name="concept">ディープ・ニューラル・ネットワークの重要概念</a>

ディープラーニングは、他の一般的な隠れ層のニューラルネットワークとは、**深層**であるという点で異なります。データは、数多くのノードから成る層を、複数の段階にわたるパターン認識を行いながら通過します。

従来の機械学習の場合、ネットワークの層は薄く、入力層と、出力層がそれぞれ1層づつあるのみです。多ければ、その中間に隠れ層がもう一つあるくらいです。ネットワークが4層以上（入力層と出力層を含む）だと、「ディープ（深層）」ラーニングと呼ばれます。したがって、厳密にいえば、ディープとは、隠れ層が2つ以上あるものと定義されます。

ディープラーニング・ネットワークでは、各ノード層は、前の層から受けた出力を基にして新しく別の特徴一式でトレーニングします。ニューラルネットワーク内を進めば進むほど、ノードはさらに複雑な特徴を認識できるようになります。この理由は、各層は、前の層が処理した特徴を集めて、再結合するからです。

![Alt text](../img/feature_hierarchy.png)

これは、**特徴の階層（feature hierarchy）**と呼ばれ、複雑さと抽象さの度合の階層なのです。このため、ディープラーニング・ネットワークは、[非線形機能（nonlinear functions）](../glossary.html#nonlineartransformfunction)を通過する何十億ものパラメータを持ち、大規模で高次元のデータセットを処理することができるのです。

とりわけ、これらのネットワークは、**まだラベル付与や構造化がされていないデータ**にある潜在的な構造を発見することが可能です。構造化されていないデータは、*生のメディア（raw media）*とも呼ばれ、写真、絵、テキスト、録画ビデオ、録音音声などが例です。従って、ディープラーニングが最も得意とすることの一つは、実際の社会から取ってきたラベルの付いていない生のメディアを処理し、クラスタリングすることで、いまだかつて誰も関連性データベースでオーガナイズしたり名前を付けたことのないデータから類似性や例外などを識別します。

例えば、ディープラーニングは、百万もの画像を取り入れ、類似性に基づいて、猫の写真、砕氷器の写真、自分の祖母の写った写真などを別々にクラスタリングし、処理することもできます。これがいわゆるsmart photoアルバムと呼ばれるものの基礎です。

同じ方法を他のデータタイプにも適用してみましょう。ディープラーニングは、生のテキストである電子メール、新しい記事をクラスタリングすることもあります。怒り心頭の苦情メール、顧客が満足感を伝えたメール、スパムボット、などの異なるメッセージの3タイプがベクトル空間に別々にクラスタリングされて配置されるというような例が挙げられます。これを様々なメッセージングフィルターのベースとして活用し、顧客関係管理（CRM）に利用することができます。この方法をボイスメッセージにも適用することができます。
時系列を入れると、データは、正常/健康的行動と異常/危険な行動にクラスタリングされるかもしれません。時系列データがスマートフォンによって生成されているのであれば、ユーザーの健康や習慣に関する分析が提供されるでしょう。自動生成されていれば、致命的な健康の悪化を回避するのに役立つかもしれません。  

ディープラーニング・ネットワークは、従来の機械学習アルゴリズムと異なり、人間の関与なしで、**自動特徴抽出（*automatic feature extraction）**します。特徴抽出は、データ科学者のチームが何年もかけて行う作業であるということを考えれば、ディープラーニングは、専門家に数に限りがある現状に対応するには良い方法ではないでしょうか。小規模の科学チームだと、作業できる量に限界がありますが、この作業をパワーアップしてくれます。

ラベルが付いていないデータをトレーニングしているとき、ディープネットワークにあるそれぞれのノードの層は、そこからサンプルを引き出す入力を繰り返し再構成しようとして、自動的に特徴を学習します。ネットワークによる推測と入力データ自体の確率分布との差異を最小化するためです。例えば、制限付きボルツマン・マシンはこの方法を使って再構成します。  

その過程で、これらのネットワークは、ある特定の関連した特徴と最適な結果の相関関係を認識することを学習します。再構成している場合でも、ラベル付きデータを処理している場合でも、特徴信号とそれらの特徴が表しているものの関係を見つけ出します。

ラベル付きのデータでトレーニングを受けたディープラーニング・ネットワークは、その後、まだ構造化されていないデータに適用することができます。そして、機械学習ネットワークよりも、ずっと多くの入力情報を受けることが可能です。これは、ニューラルネットワークの能力をより高めたい場合にお勧めの方法です。ネットワークがトレーニングできるデータが多ければ多いほど、より正確に判断できるようになるからです。（優秀でないアルゴリズムでも、たくさんのデータでトレーニングすれば、少しのデータでトレーニングした優秀なアルゴリズムより能力は高くなります。）ディープラーニングには、膨大な量のラベルが付与されていないデータを処理し、学習する能力があるため、従来のアルゴリズムよりずいぶん有利です。

ディープラーニング・ネットワークは、最後に出力層で作業を終えます。記号論理、ソフトマックスなど、ある結果やラベルに尤度を割り当てます。我々はそれを予測分析と呼びますが、大まかな意味での予測です。例えば、画像という形態で、生のデータが与えられると、ディープラーニング・ネットワークは入力されたデータが、90%の確率で人間を表していると判断するでしょう。

## <a name="forward">例：フィードフォワードネットワーク</a>

弊社にとって、ニューラルネットワークの目標は、できるだけ早くエラーを最小限にさせることです。まるでレースを走っているようなものなのです。このレースは円形トラック上で行われ、同じポイントを繰り返し通過します。レースのスタートラインでは、重みが初期化され、フィニッシュラインでは、パラメターが精確な分類や予測を行うことができるようになっています。

レース自体には、多くの段階があり、各段階はそれぞれ、その前後の段階と似ています。ランナーのように、完了するまで何度も同じアクションを繰り返します。そして、ニューラルネットワークの各段階では、推測、エラー測定、重みの微調整、係数の漸進的な調整が行われます。

重みの集合体は、開始時でも完了後でも、モデルとも呼ばれています。これは、重みがデータと正解データ（ground truth）との関係をモデル化する試みを行うからです。モデルは、スタート時点では、あまり能力は高くありませんが、最後の段階では、向上しています。これは、ニューラルネットワークが時を追うごとにパラメータを更新するからです。

そして、最初はニューラルネットワークは全く無知だからです。ニューラルネットワークは、精確に推測するために、どのような重みやバイアスを結合すれば入力を適切に翻訳できるのかを知りません。このため、まずは推測から開始し、間違いが生じるとそれを学習し、それを基にして、よりよい推測ができるように試みます。（ニューラルネットワークは、仮説を繰り返しテストする科学的方法のと考えていただいて構いません。目隠しされた科学的方法といえます。）

説明が最も簡単なアーキテクチャーであるフィードフォワード（順伝播型）ニューラルネットワークを例に挙げてみましょう。フィードフォワードニューラルネットワークの場合、学習している間に以下のようなことが起こります。

ネットワークに入力され、係数、あるいは重みが、この入力をネットワークが最後に作る推測のグループにマッピングします。

    input * weight = guess

入力に重みが付与された後、入力が何かについての推測が行われます。その後、ニューラルネットワークは、その推測を正解データと比較します。つまり専門家の人に「この回答で正解でしょうか？」と聞いているようなものです。

    ground truth - guess = error

ネットワークの推測と正解データの違いが*error*と出ました。ネットワークは、そのエラーを測定し、エラーを伴ってモデルに戻り、エラーになった分の重みを調整します。

    error * weight's contribution to error = adjustment

上記の3つの疑似数式が、ニューラルネットワークに重要な関数です。入力データのスコア付け、ロスの計算、モデルの更新、そしてこの同じ3段階をまた繰り返します。ニューラルネットワークとは、いわば修正を伴うフィードバックを送るループ状のものであり、正しく行われた推測をサポートし、間違った推測には、重みに修正を加えることにより対応します。

上記の最初のステップを見てみましょう。

### <a name="multiple">多重線形回帰</a>

ニューラルネットワークは、まるで生物であるかのような名前を持っていますが、他の機械学習アルゴリズムと同様、人工的で、単に数式やコードによって構成されたものに過ぎません。実際、統計学で最初に学ぶ*線形回帰*を知っている人は誰もがニューラルネットワークの仕組みを理解することができます。最も簡単な数式では、線形回帰は、以下のように表されます。

        Y_hat = bX + a

`Y_hat`は、判断された出力結果で、Xが入力データ、bは傾斜で、aは2次元グラフの縦のX軸の切片です。（より具体的に言うと、Xは放射能への被ばく量でYは癌に罹るリスク、Xは毎日行う腕立て伏せの回数でYはベンチプレスで持ち上げられる全重量、Xは肥料の量でYは収穫量などが例として挙げられます。）X軸のどの位置にあろうが、Xにユニットを追加するたびに、従属変数のYはそれに比例して増加すると推測できます。この上下に変動する簡単な2つの変数の関係がベースとなります。

次に多重線形回帰で、入力変数が多くあり、出力変数が1つである場合を想像してください。それは、一般に以下のように表されます。

        Y_hat = b_1*X_1 + b_2*X_2 + b_3*X_3 + a

（上述した収穫量の例をさらに広げて考えていくと、日光の量、降水量、肥料の変数すべてが、`Y_hat`の結果に反映されます。）

このような多重線形回帰の計算が、ニューラルネットワークのすべてのノードで実行されています。各層のそれぞれのノードでは、その前の層の各ノードからの入力とその他のノードからの入力の新しい組み合わせで結合します。入力は、それぞれの異なる係数に従って異なる割合で混ぜ合わされ、次の層の各ノードに送られます。この方法で、ネットワークは、エラーの減少を試みながら、どの入力の結合が重要なのかをテストします。

いったん、複数のノードへの入力が合計されて`Y_hat`まで行くと、非線形機能を通過します。なぜかというと、各ノードが多重線形回帰を行うと、Xが増加するごとに無限に`Y_hat`が直線的に増加しますが、これは我々が目的とすることとは異なるからです。

我々が各ノードで構築しようとしているのは（神経細胞のような）スイッチで、ネットワークの最終決定に反映させるために入力信号を通過させるか、させないかに応じてオン、オフと切り替えができるものです。

スイッチが設けられると、次は分類作業に取り掛かります。入力信号を受けて、ノードはそれを十分であるか否か、つまり、オンかオフに分類しますが、バイナリの決定は、1か0で表現します。[ロジスティック回帰（logistic regression）](#logistic)は、非線形関数で、入力情報を0と1の間の空間に翻訳します。

各ノードでの非線形変換には、一般的にはロジスティック回帰に似たS字型関数を使います。この関数は、シグモイド（アルファベットの「S」のギリシア語）、tanh、hard tanh、などという名前で知られていますが、各ノードの出力を形成しています。すべてのノードの出力は、それぞれ0と1の間のS字型の空間に押し込められ、次の層では入力情報となります。層から層へと同様にして処理され、最後には信号が最後の層に着き、決定されます。

### 勾配降下

エラーに応じて重みを調整する最適化機能は一般に「勾配降下（gradient descent）」と呼ばれています。

勾配（gradient）とは、別名を傾斜と言いますが、この傾斜は、x-y軸のグラフでは、二つの変数がどのように関連しているかを表します。つまりX軸の変数の移動に伴うY軸の値の上下の変動で、時間の経過に伴う貨幣価値の変動などが例です。ここでは、傾斜はネットワークのエラーと重み1つの関係を表したものですから、重みが調整されるとエラーがどう変わるかという関係です。

これをさらに具体的に言うと、どの重みが最小限のエラーを出すか、ということです。どの重みが入力データの信号を正確に表現し、正しく分類できる解釈を行うのか、どの重みが入力した画像で「nose（鼻）」と解釈でき、フライパンではなく顔としてラベルを付与するべきであると分かるのか、ということです。

ニューラルネットワークが学習を重ねるにつれて、信号を正確にマッピングできることを目指して、数多くの重みを徐々に調整していきます。ネットワークの*エラー*とそれらの*重み*の関係は微分係数*dE/dw*で表され、これは重みのわずかな変化がわずかなエラーの変化を引き起こす程度を測定します。

重みは、多くの変換を伴うディープネットワークの要因の1つに過ぎません。重みの信号は、活性化関数を通過し、いくつかの層で合計されるため、我々は、[微分積分の連鎖律（chain rule of calculus）](https://en.wikipedia.org/wiki/Chain_rule)を使用することによって、ネットワークの活性化機能や出力の段階に戻り、最終的には修正したい重みに行き着き、そのエラー全体との関係を調べます。

微分積分の連鎖律は以下のように表されます。

![Alt text](../img/chain_rule.png)

そして、フィードフォワードニューラルネットワークの場合、ネットワークのエラーと重み1つは、以下のような式で表されます。

![Alt text](../img/backprop_chain_rule.png)

つまり、*エラー*と*重み*の二つの変数の関係には、重みが通過する*活性化*が媒介しているため、*重み*の変化が*エラー*の変化にどのように影響しているかを計算するには、まず最初に*活性化*の変化が*エラー*の変化にどのように影響しているか、ということと*重み*の変化が*活性化*の変化にどのように影響しているかを計算する必要があります。  

ディープラーニングの学習の本質は、重みが生み出すエラーが最小限になるまでモデルの重みを調整することなのです。

## <a name="logistic">ロジスティック回帰</a>

数多くの層で構成されたディープ・ニューラル・ネットワークの最後の層には特別な役割があります。ラベル付き情報が入力されると出力層は、各exampleを分類し、最も正解である可能性の高いラベルを付与します。出力層の各ノードはラベル1つを表現しており、ノードは、その前の層からの入力情報とパラメーターから受け取る信号の強度に応じてスイッチが入ったり切れたりします。

それぞれの出力ノードは、バイナリ出力値が0か1かの2つの可能性のある結果を産出します。[入力変数は、ラベルが付与できるかできないかのどちらかだからです](https://en.wikipedia.org/wiki/Law_of_excluded_middle)。例えば、少しだけ妊娠している、などという状態はあり得ないのです。  

ニューラルネットワークは、ラベル付きのデータを処理するとバイナリ出力を産出しますが、受け取る入力情報は多くの場合、連続的なものです。つまり、ネットワークの受け取る信号は、解決しようとする問題に応じて数が異なる測定基準なども含む一連の値なのです。

例えば、リコメンデーションエンジンは、広告を提供するか否かを、0か1かを選択することにより決断し、バイナリ出力を行わなければなりません。ところが、その決断を行うために受け取る入力情報自体は、Amazonで先週どのくらいの額を支払ったのか、またどのくらいの頻度でそのサイトへ行くか、などです。

ですから、出力層は、おむつに$67.59の支払い、ウェブサイトに15回行ったなどといった内容の信号を0と1の間に押し込むことにより、入力にラベルを付けるか付けないかの可能性を表さなければなりません。

連続的な信号をバイナリ出力に変換していく仕組みは、ロジスティック回帰と呼ばれます。この名前は少し紛らわしいものになっています。というのは、ここでのロジスティック回帰は、多くの人になじみのある線的な回帰というより、分類に使われているからです。入力情報のセットがラベルに一致する可能性を計算します。  

![Alt text](../img/logistic_regression.png)

上記の数式を見てください。

連続的な入力情報が可能性を表すものであるには、数値結果が正の数にならなければなりません。可能性が負の数で表されることはないからです。このため、入力情報が*e*の指数として分母にあるのです。指数であるということは、結果は必ず0より大きくなります。次に、*e*の指数と分数の1/1の関係を考えてみましょう。可能性の最高値は、1であり、これ以上になることはあり得ません。（これは120%確かだと言えましょう！）

ラベルを大きくする要因である入力情報の*x*が大きくなればなるほど、*e to the x*の値がゼロに近づき、分数の値が1/1、つまり100%に近づきます。そして、絶対的にラベルが適用される値に（達することはありませんが）限りなく近づきます。入力情報が出力情報と負の相関関係にある場合、*e*の指数に負記号があるため、正の数に変わります。そして、負の信号が大きくなればなるほど、*e to the x*が大きくなり、全体の分数の値がゼロの近づいていきます。

次に、*x*は指数でなく、すべての重みとその入力情報の総和であると想像してみてください。すべての信号がネットワークを通過します。分類を行うニューラルネットワークの出力層のロジスティック回帰層に入力しているものがこれなのです。

この層で、exampleが付与されるラベルの値である1以上、またはそれ以下に閾値を設定することができます。好きな値に閾値を設定できるのです。エラーとして発生させたいのがどちら側かによって、誤検出の数を増やすのを低めの閾値に設定した場合か高めに設定した場合のどちらか決めることができるでしょう。

## <a name="ai">ニューラルネットワーク＆人口知能（AI）</a>

一部では、ニューラルネットワークは「強制的な」AIであると思われています。真っ白な状態からハンマーで打ち込むように繰り返しトレーニングして正確なモデルへと形作っていくからです。この方法は効果的ではありますが、モデリングへのアプローチとしては、出力と入力の間の機能的依存関係について想定することができないため、非効率的であると思われるようです。

とはいえ、勾配降下とは最適なマッチを見つけるまで他の重みと結合し直しているわけではありません。その作業過程では、関連ある重み空間が縮小されるため、更新や計算の回数を何桁分も省くことができるのです。

## <a name="intro">その他の説明ガイドリソース</a>

ディープラーニングの初心者の方には、以下のチュートリアルやビデオがフィードフォワードニューラルネットワークの基礎を学ぶのに役に立ちます。是非お役立てください。

* [Restricted Boltzmann Machines（制限付きボルツマン・マシン）](ja-restrictedboltzmannmachine)
* [Eigenvectors, PCA, Covariance and Entropy（固有ベクトル、PCA、共分散、エントロピー](ja-eigenvector)
* [Glossary of Deep-Learning and Neural-Net Terms（ディープラーニングとニューラルネットワーク用語集）](../glossary)
* [Convolutional Networks（畳み込みネットアーク）](ja-convolutionalnets)
* [Recurrent Networks and LSTMs（リカレントネットワークとLSTM）](ja-lstm)
* [Word2vec and Natural-Language Processing（Word2vecと自然言語処理）](ja-word2vec)
* [Deeplearning4j Examples via Quickstart（クイックスタートで見るDeeplearning4jの例）](ja-quickstart)
* [Neural Networks Demystified（ニューラルネットワークの解明ガイド）](https://www.youtube.com/watch?v=bxe2T-V8XRs)（7本の連続ビデオ）
* [A Neural Network in 11 Lines of Python（パイソンの11のコマンドラインで実装するニューラルネットワーク）](https://iamtrask.github.io/2015/07/12/basic-python-network/)
* [A Step-by-Step Backpropagation Example（ステップごとに見る誤差逆伝播法の例）](http://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)
